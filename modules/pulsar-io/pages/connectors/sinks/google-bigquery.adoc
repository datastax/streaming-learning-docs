= Google BigQuery
:connectorName: bigquery-sink
:connectorType: bigquery
:page-tag: bigquery,sink-connector


https://cloud.google.com/bigquery[Google BigQuery] is a fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence. BigQuery's serverless architecture lets you use SQL queries to answer your organization's biggest questions with zero infrastructure management. BigQuery's scalable, distributed analysis engine lets you query terabytes in seconds and petabytes in minutes.

BigQuery {pulsar-short} Sink is not integrated with BigQuery directly. It uses {pulsar-short}’s built-in https://pulsar.apache.org/docs/adaptors-kafka/[Kafka Connect adapter] library to transform message data into a Kafka compatible format. Then the https://docs.confluent.io/kafka-connectors/bigquery/current/kafka_connect_bigquery_config.html[Kafka Connect BigQuery Sink] is used as the actual BigQuery integration. The adaptor provides a flexible and extensible framework for data transformation and processing. It supports various data formats, including JSON, Avro, and Protobuf, and enables users to apply transformations on the data as it is being streamed from {pulsar-short}.

You will notice references to Kafka throughout the configuration. *You don’t need a running instance of Kafka to use this connector.* The Kafka references are used as "translation points” by this connector.

[NOTE]
====
For more information on the Kafka Connect adapter, see https://www.datastax.com/blog/simplify-migrating-kafka-to-pulsar-kafka-connect-support[Simplify migrating from Kafka to {pulsar-short} with Kafka Connect Support] and the "https://medium.com/building-the-open-data-stack/datastax-presents-snowflake-sink-connector-for-apache-pulsar-53629b196064[{company} Snowflake Sink Connector for {pulsar-reg}].
====

== Get Started

include::partial$connectors/sinks/get-started.adoc[]

== Managing the Connector

include::partial$connectors/sinks/manage.adoc[]

== Monitoring the Connector

include::partial$connectors/sinks/monitoring.adoc[]

== Connector Reference

The BigQuery sink has multiple sets of parameters: the {product} parameters, the Kafka Connect Adapter parameters, and the Google BigQuery parameters. Each set of parameters provides a way to coordinate how data will be streamed from {pulsar-short} to BigQuery.

=== {product}

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$connectors/sinks/astra.csv[]
|===

=== Kafka Connect Adapter Configuration (configs)

These values are provided in the `configs` area.

For source code for these configuration, see `https://github.com/apache/pulsar/blob/master/pulsar-io/kafka-connect-adaptor/src/main/java/org/apache/pulsar/io/kafka/connect/PulsarKafkaConnectSinkConfig.java[PulsarKafkaConnectSinkConfig.java]`.

[%header,cols="1,1,1,4"]
|===
| Name | Required | Default | Description

| kafkaConnectorSinkClass
| yes
|
a| A Kafka-connector sink class to use. Unless you've developed your own, use the value `com.wepay.kafka.connect.bigquery.BigQuerySinkConnector`.

| offsetStorageTopic
| yes
|
| Pulsar topic to store offsets at. This is an additional topic to your topic with the actual data going to BigQuery.

| sanitizeTopicName
| yes
|
a| Some connectors cannot handle Pulsar topic names like `persistent://a/b/topic`, and they won't sanitize the topic name themselves.
If enabled, all non alpha-digital characters in topic name are replaced with underscores.
In some cases this may result in topic name collisions (`topic_a` and `topic.a` both resolve to `topic_a`).

This value _must_ be `true`.
Any other value causes an error.

| batchSize
| no
| 16384
| Size of messages in bytes the sink will attempt to batch messages together before flush.

| collapsePartitionedTopics
| no
| false
| Supply Kafka record with topic name without -partition- suffix for partitioned topics.

| kafkaConnectorConfigProperties
| no
| `{}`
| A key/value map of config properties to pass to the Kafka connector. See the reference table below.

| lingerTimeMs
| no
| 2147483647L
| Time interval in milliseconds the sink will attempt to batch messages together before flush.

| maxBatchBitsForOffset
| no
| 12
| Number of bits (0 to 20) to use for index of message in the batch for translation into an offset. 0 to disable this behavior (Messages from the same batch will have the same offset which can affect some connectors.)

| topic
| yes
|
| The Kafka topic name that is passed to the Kafka sink.

| unwrapKeyValueIfAvailable
| no
| true
| In case of Record<KeyValue<>> data use key from KeyValue<> instead of one from Record.

| useIndexAsOffset
| no
| true
| Allows use of message index instead of message sequenceId as offset, if available. Requires AppendIndexMetadataInterceptor and exposingBrokerEntryMetadataToClientEnabled=true on brokers.

| useOptionalPrimitives
| no
| false
| Pulsar schema does not contain information whether the Schema is optional, Kafka's does. This provides a way to force all primitive schemas to be optional for Kafka.
|===

=== Google BigQuery Configuration (kafkaConnectorConfigProperties)

These values are provided in the `kafkaConnectorConfigProperties` area.

For the source code for these configurations, see `https://github.com/confluentinc/kafka-connect-bigquery/blob/master/kcbq-connector/src/main/java/com/wepay/kafka/connect/bigquery/config/BigQuerySinkConfig.java[BigQuerySinkConfig.java]`.

[%header,cols="1,1,1,4"]
|===
| Name | Required | Default | Description

| allBQFieldsNullable
| no
| false
| If true, no fields in any produced BigQuery schemas are `REQUIRED`.
All non-nullable Avro fields are translated as `NULLABLE` (or `REPEATED`, if arrays).

| allowBigQueryRequiredFieldRelaxation
| no
| false
| If true, fields in BigQuery Schema can be changed from `REQUIRED` to `NULLABLE`.

| allowNewBigQueryFields
| no
| false
| If true, new fields can be added to BigQuery tables during subsequent schema updates.

| allowSchemaUnionization
| no
| false
a| If true, the existing table schema (if one is present) is unionized with new record schemas during schema updates.

If false, the record of the last schema in a batch is used for any necessary table creation and schema update attempts.

Setting `allowSchemaUnionization` to false _and_ `allowNewBigQueryFields` and `allowBigQueryRequiredFieldRelaxation` to true is equivalent to setting `autoUpdateSchemas` to true in older (pre-2.0.0) versions of this connector.
In this case, if BigQuery raises a schema validation exception or a table doesn't exist when a writing a batch, the connector tries to remediate by required field relaxation and/or adding new fields.

If `allowSchemaUnionization`, `allowNewBigQueryFields`, and `allowBigQueryRequiredFieldRelaxation` are all true, then the connector creates or updates tables with a schema whose fields are a union of the existing table schema fields and the fields present in all of the records of the current batch.

The key difference is that with unionization disabled, new record schemas have to be a superset of the table schema in BigQuery.

`allowSchemaUnionization` is a useful tool for parsing.
For example, if you'd like to remove fields from data upstream, the updated schemas still work in the connector. It is similarly useful when different tasks see records whose schemas contain different fields that are not in the table.

However, be aware that if `allowSchemaUnionization` is set to true, and some bad records are in the topic, then the BigQuery schema can be permanently changed.
This presents two issues:

* Since BigQuery doesn't allow columns to be dropped from tables, they add unnecessary noise to the schema.
* Since BigQuery doesn't allow column types to be modified, they can break downstream pipelines where well-behaved records have schemas whose field names overlap with the accidentally-added columns in the table, but the types don't match.

| autoCreateBucket
| no
| true
| Whether to automatically create the given bucket if it does not exist.

| autoCreateTables
| no
| false
| Automatically create BigQuery tables if they don't already exist

| avroDataCacheSize
| no
| 100
| The size of the cache to use when converting schemas from Avro to Kafka Connect.

| batchLoadIntervalSec
| no
| 120
| The interval, in seconds, in which to attempt to run GCS to BigQuery load jobs. Only relevant if `enableBatchLoad` is configured.

| bigQueryMessageTimePartitioning
| no
| false
| Whether or not to use the message time when inserting records. Default uses the connector processing time.

| bigQueryPartitionDecorator
| no
| true
| Whether or not to append partition decorator to BigQuery table name when inserting records. Default is true. Setting this to true appends partition decorator to table name (e.g. table$yyyyMMdd depending on the configuration set for bigQueryPartitionDecorator). Setting this to false bypasses the logic to append the partition decorator and uses raw table name for inserts.

| bigQueryRetry
| no
| 0
| The number of retry attempts made for a BigQuery request that fails with a backend error or a quota exceeded error.

| bigQueryRetryWait
| no
| 1000
| The minimum amount of time, in milliseconds, to wait between retry attempts for a BigQuery backend or quota exceeded error.

| clusteringPartitionFieldNames
| no
|
| Comma-separated list of fields where data is clustered in BigQuery.

| convertDoubleSpecialValues
| no
| false
| Designates whether +Infinity is converted to Double.MAX_VALUE and whether -Infinity and NaN are converted to Double.MIN_VALUE to ensure successful delivery to BigQuery.

| defaultDataset
| yes
|
| The default dataset to be used

| deleteEnabled
| no
| false
| Enable delete functionality on the connector through the use of record keys, intermediate tables, and periodic merge flushes. A delete will be performed when a record with a null value (that is–a tombstone record) is read. This feature will not work with SMTs that change the name of the topic.

| enableBatchLoad
| no
| empty
| Beta Feature. Use with caution. The sublist of topics to be batch loaded through GCS.

| gcsBucketName
| no
| empty
| The name of the bucket where Google Cloud Storage (GCS) blobs are located. These blobs are used to batch-load to BigQuery. This is applicable only if `enableBatchLoad` is configured.

| includeKafkaData
| no
| false
| Whether to include an extra block containing the Kafka source topic, offset, and partition information in the resulting BigQuery rows.

| intermediateTableSuffix
| no
| `.tmp`
| A suffix that will be appended to the names of destination tables to create the names for the corresponding intermediate tables. Multiple intermediate tables may be created for a single destination table, but their names will always start with the name of the destination table, followed by this suffix, and possibly followed by an additional suffix.

| kafkaDataFieldName
| no
|
| The Kafka data field name. The default value is null, which means the Kafka Data field will not be included.

| kafkaKeyFieldName
| no
|
| The Kafka key field name. The default value is null, which means the Kafka Key field will not be included.

| keyfile
| yes
|
a| Can be either a string representation of the Google credentials file or the path to the Google credentials file itself.

When using the Astra Streaming UI, the string representation must be used. If using pulsar-admin with Astra Streaming, either the representation or file can be used.

| keySource
| yes
| `FILE`
a| Determines whether the keyfile configuration is the path to the credentials JSON file or to the JSON itself. Available values are `FILE` and `JSON`.

When using the Astra Streaming UI, JSON will be the only option. If using pulsar-admin with Astra Streaming, either the representation or file can be used.

| name
| yes
|
| The name of the connector. Use the same value as Pulsar sink name.

| mergeIntervalMs
| no
| `60_000L`
| How often (in milliseconds) to perform a merge flush, if upsert/delete is enabled. Can be set to -1 to disable periodic flushing.

| mergeRecordsThreshold
| no
| `-1`
| How many records to write to an intermediate table before performing a merge flush, if upsert/delete is enabled. If set to `-1`, then record count-based flushing is disabled.

| project
| yes
|
| The BigQuery project to write to

| queueSize
| no
| `-1`
| The maximum size (or `-1` for no maximum size) of the worker queue for BigQuery write requests before all topics are paused. This is a soft limit; the size of the queue can go over this before topics are paused. All topics resume once a flush is triggered or the size of the queue drops under half of the maximum size.

| sanitizeTopics
| yes
| false
a| Designates whether to automatically sanitize topic names before using them as table names. If not enabled, topic names are used as table names.

The only accepted value is `false`. Providing any other value will result in an error.

| schemaRetriever
| no
| `com.wepay.kafka.connect.bigquery.retrieve.IdentitySchemaRetriever`
| A class that can be used for automatically creating tables and/or updating schemas.

| threadPoolSize
| no
| 10
| The size of the BigQuery write thread pool. This establishes the maximum number of concurrent writes to BigQuery.

| timePartitioningType
| no
| `DAY`
| The time partitioning type to use when creating tables. Existing tables will not be altered to use this partitioning type. Valid Values: (case insensitive) [MONTH, YEAR, HOUR, DAY]

| timestampPartitionFieldName
| no
|
| The name of the field in the value that contains the timestamp to partition by in BigQuery and enable timestamp partitioning for each table. Leave this configuration blank, to enable ingestion time partitioning for each table.

| topic2TableMap
| no
|
a| Optional map of topics to tables in the format of comma-separated tuples, such as `<topic-1>:<table-1>,<topic-2>:<table-2>,...`

Because `sanitizeTopicName` must be `true`, any alphanumeric character in topic names are replaced with underscores.
Keep this in mind when creating the mapping to avoid overlapping names.
For example, if the topic name is provided as `persistent://a/b/c-d`, then the mapping topic name would be `persistent___a_b_c_d`.

| topics
| yes
|
| A list of Kafka topics to read from. Use the same name as the Pulsar topic.
Only provide the topic name, not the whole address.

| upsertEnabled
| no
| false
| Enable upsert functionality on the connector through the use of record keys, intermediate tables, and periodic merge flushes. Row-matching will be performed based on the contents of record keys. This feature won't work with SMTs that change the name of the topic.
|===
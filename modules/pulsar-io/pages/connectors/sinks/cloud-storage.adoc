= Cloud Storage
:connectorName: cloud-storage-sink
:connectorType: cloud-storage
:page-tag: cloud storage,sink-connector, aws, azure, gcp

Each public cloud has different ways of persisting data to their storage systems. Each cloud has their own way of formatting and storing the bytes. The Cloud Storage sink connector is a general interface to a chosen cloud storage, that exports data from a {pulsar-short} topic to the given system following a desired format.

The cloud storage system supported are:

* https://cloud.google.com/storage[Google's Cloud Storage (GCP)]
* https://azure.microsoft.com/en-us/products/storage/blobs[Azure Blob Store (Azure)]
* https://aws.amazon.com/s3/[Amazon Web Services S3 (AWS)]

== Get Started

include::partial$connectors/sinks/get-started.adoc[]

== Data format types

The Cloud Storage sink connector provides multiple output format options, including JSON (default), Avro, Bytes, or Parquet.
There are some limitations for certain formats, as explained in the following sections.

=== {pulsar-short} Schema types supported by the writers

[%header,format=csv,cols="1,^1,^1,^1,^1"]
|===
include::example$connectors/sinks/cloud-storage/data-format.csv[]
|===

____
*The JSON writer will try to convert data with a String or Bytes schema to JSON-format data if convertable.

**The Protobuf schema is based on the Avro schema. It uses Avro as an intermediate format, so it may not provide the best effort conversion.

\*** The ProtobufNative record holds the Protobuf descriptor and the message. When writing to Avro format, the connector uses avro-protobuf to do the conversion.
____

=== Supported withMetadata configurations for writer formats

[%header,format=csv,cols="1,^1",width="50%"]
|===
include::example$connectors/sinks/cloud-storage/with-meta-data.csv[]
|===

____
*When using Parquet with PROTOBUF_NATIVE format, the connector will write the messages with the DynamicMessage format. When withMetadata is set to true, the connector will add __message_metadata__ to the messages with PulsarIOCSCProtobufMessageMetadata format.
____

.PROTOBUF_NATIVE example
[%collapsible]
====
For example, if a message `User` has the following schema:

[source,protobuf]
----
syntax = "proto3";
message User {
 string name = 1;
 int32 age = 2;
}
----

When `withMetadata` is set to true, the connector writes the message `DynamicMessage` with the following schema:

[source,protobuf]
----
syntax = "proto3";
message PulsarIOCSCProtobufMessageMetadata {
 map<string, string> properties = 1;
 string schema_version = 2;
 string message_id = 3;
}
message User {
 string name = 1;
 int32 age = 2;
 PulsarIOCSCProtobufMessageMetadata __message_metadata__ = 3;
}
----
====

== Skip unsupported messages

By default, when the connector receives a message with a non-supported schema type, the connector will fail the message.
If you want to skip the non-supported messages, you can set `skipFailedMessages` to true.

== Dead-letter topics

If a message fails to send to a Cloud Storage sink, the connector can send the message to a https://pulsar.apache.org/docs/en/concepts-messaging/#dead-letter-topic[dead-letter topic] instead, if a dead-letter topic is assigned.

To use a dead-letter topic, set `skipFailedMessages` to `false` in the cloud provider config.
Then, using either `pulsar-admin` or curl, set `--max-redeliver-count` and `--dead-letter-topic`.

== Managing the Connector

include::partial$connectors/sinks/manage.adoc[]

== Monitoring the Connector

include::partial$connectors/sinks/monitoring.adoc[]

== Connector Reference

With the Cloud Storage Sink there are two sets of parameters: {product} parameters and cloud storage provider parameters.

=== {product} parameters for Cloud Storage Sink

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$connectors/sinks/astra.csv[]
|===

=== Cloud storage provider parameters for Cloud Storage Sink

Set your cloud storage provider and other required values in the `configs` area.

[tabs]
======
Google Cloud Storage::
+
--
[%header,format=csv,cols="2,1,1,3"]
|===
include::example$connectors/sinks/cloud-storage/gcp-gcs.csv[]
|===
--

AWS S3 Storage::
+
--
The suggested permission policies for AWS S3 are:

- `s3:AbortMultipartUpload`
- `s3:GetObject*`
- `s3:PutObject*`
- `s3:List*`

If you don't want to provide a region in the configuration, then enable the `s3:GetBucketLocation` permission policy as well.

[%header,format=csv,cols="2,1,1,3"]
|===
include::example$connectors/sinks/cloud-storage/aws-S3.csv[]
|===
--

Azure Blob Storage::
+
--
[%header,format=csv,cols="2,1,1,3"]
|===
include::example$connectors/sinks/cloud-storage/azure-blob.csv[]
|===
--
======
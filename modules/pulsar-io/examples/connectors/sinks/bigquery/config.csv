Name,Required,Default,Description
kafkaConnectorSinkClass,true,,"A Kafka-connector sink class to use. Unless you've developed your own, use the value ""com.wepay.kafka.connect.bigquery.BigQuerySinkConnector""."
offsetStorageTopic,true,,Pulsar topic to store offsets at. This is an additional topic to your topic with the actual data going to BigQuery.
sanitizeTopicName,true,,"Some connectors cannot handle Pulsar topic names like persistent://a/b/topic and do not sanitize the topic name themselves. If enabled, all non alpha-digital characters in topic name will be replaced with underscores. In some cases this may result in topic name collisions (topic_a and topic.a will become the same)

This value MUST be set to `true`. Any other value will result in an error."
batchSize,false,16384,Size of messages in bytes the sink will attempt to batch messages together before flush.
collapsePartitionedTopics,false,false,Supply Kafka record with topic name without -partition- suffix for partitioned topics.
kafkaConnectorConfigProperties,false,{},A key/value map of config properties to pass to the Kafka connector. See the reference table below.
lingerTimeMs ,false,2147483647L,Time interval in milliseconds the sink will attempt to batch messages together before flush.
maxBatchBitsForOffset,false,12,Number of bits (0 to 20) to use for index of message in the batch for translation into an offset. 0 to disable this behavior (Messages from the same batch will have the same offset which can affect some connectors.)
topic,true,,The Kafka topic name that is passed to the Kafka sink.
unwrapKeyValueIfAvailable ,false,true,In case of Record<KeyValue<>> data use key from KeyValue<> instead of one from Record.
useIndexAsOffset,false,true,"Allows use of message index instead of message sequenceId as offset, if available. Requires AppendIndexMetadataInterceptor and exposingBrokerEntryMetadataToClientEnabled=true on brokers."
useOptionalPrimitives,false,false,"Pulsar schema does not contain information whether the Schema is optional, Kafka's does. This provides a way to force all primitive schemas to be optional for Kafka."
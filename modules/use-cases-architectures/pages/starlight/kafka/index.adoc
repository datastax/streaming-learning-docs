= Getting started with the {kafka-for-astra} extension
:navtitle: {kafka-for-astra}
:description: Learn how to get started using the {kafka-for-astra} extension with {pulsar-reg} and get hands on with Kafka producer and consumer interacting with a topic.

{kafka-for-astra} brings the native Apache Kafka(R) protocol support to {pulsar-reg} by introducing a Kafka protocol handler on {pulsar-short} brokers. By adding the {kafka-for-astra} protocol handler to your existing {pulsar-short} cluster, you can migrate your existing Kafka applications and services to {pulsar-short} without modifying the code.

If source code is your thing, visit the https://github.com/datastax/starlight-for-kafka[project's repo on GitHub].

== Architecture reference

If you would like to get deep into how {kafka-for-astra} works, xref:starlight-for-kafka:ROOT:index.adoc[read the docs].

image:s4k-architecture.png[{kafka-for-astra} Architecture]

== Establishing the Kafka protocol handler

Before a Kafka client can interact with your {pulsar-short} cluster, you need the {kafka-for-astra} protocol handler installed in the cluster.
Installation looks a bit different depending on where your {pulsar-short} cluster is running.
Choose the option that best fits your needs.

[tabs]
====
{product}::
+
--

If you want a working Kafka extension as quickly as possible, this is your best bet.
This is also a good option for those that already have a streaming tenant and are looking to extend it.

. Sign in to your {product-short} account and navigate to your streaming tenant.
+
TIP: Don't have a streaming tenant? Follow our "xref:astra-streaming:getting-started:index.adoc[]" guide.

. Go to the "Connect" tab and choose the "Kafka" option.

. Click "Enable Kafka".

. A message will let you know of the additions (and restrictions) that come with using {kafka-for-astra}.

. Select the "Enable Kafka" button to confirm your understanding.

Your {product} tenant is ready for prime time! Continue to the next section of the guide to see it in action.
--
Luna Streaming::
+
--
The {kafka-for-astra} extension is included in the `luna-streaming-all` image used to deploy a Luna cluster. The Luna helm chart makes deploying the Kafka extension quite easy. Follow the "xref:luna-streaming:components:starlight-for-kafka.adoc[]" guide to create a simple {pulsar-short} cluster with the {kafka-for-astra} extension ready for use.
--
Self Managed::
+
--
Already have your own {pulsar-short} cluster? Or maybe you're using a standalone cluster? {kafka-for-astra} can easily be a part of that cluster! Follow the "xref:starlight-for-kafka:installation:starlight-kafka-quickstart.adoc[]" guide.
--
====

== Messaging with {kafka-for-astra}

{kafka-for-astra} supports quite a few different use cases. With a {pulsar-short} cluster between producers and consumers you can interchange the type of producer and consumer to fit your needs.

*The below examples are using an {product} tenant as the Kafka bootstrap server.* If you are using Luna Streaming or a self-managed tenant, switch the bootstrap server URL for your own.

=== Retrieve Kafka connection properties in {product}

In the {product} portal "Connect" tab, the "kafka" area provides important connection information.
You will need this connection information to create a working Kafka client or use the CLI.

image:kafka-client-settings.png[{product} kafka settings]

TIP: Click the clipboard icon to copy the Kafka connection values, as well as a working token to paste in code.

=== Produce and consume a message

[tabs]
======
Kafka CLI::
+
--
Download the latest Kafka distribution https://www.apache.org/dyn/closer.cgi?path=/kafka/3.3.1/kafka_2.13-3.3.1.tgz[here].
With the tarball extracted, the producer and consumer CLIs are in the 'bin' folder.

. To get started, let's set a few variables. If you've completed our "xref:astra-streaming:getting-started:index.adoc[Getting started with {product}]" guide, the below values will be a perfect fit for your existing tenant.
+
[source,shell]
----
SERVICE_URL="<REPLACE_WITH_BOOTSTRAP_SERVER_URL>"
TENANT="<REPLACE_WITH_TENANT_NAME>"
NAMESPACE="<REPLACE_WITH_NAMESPACE>"
TOPIC="<REPLACE_WITH_TOPIC>"
----

. Now let's enter those variables in Kafka's producer shell.
+
[source,shell]
----
# cd kafka_2.13-3.3.1
./bin/kafka-console-producer.sh --topic "$TENANT/$NAMESPACE/$TOPIC" --bootstrap-server "$SERVICE_URL"
----

. Type in a super memorable message and hit 'enter' to send. Press 'Ctrl-C' to exit the shell.
+
[source,shell]
----
> This is my first S4K message.
----
A new message has been produced in the provided tenant/namespace/topic and is ready for consumption.
. Start the Kafka consumer shell.
+
[source,shell]
----
# cd kafka_2.13-3.3.1
./bin/kafka-console-consumer.sh --topic "$TENANT/$NAMESPACE/$TOPIC" --from-beginning --bootstrap-server "$SERVICE_URL"
----

. The consumer should immediately find the new message and output its value.
+
[source,shell]
----
This is my first S4K message.
----

. Press 'Ctrl-C' to exit the consumer shell.
--

Kafka Client (Java)::
+
--
This example uses Maven for the project structure.
If you prefer Gradle or another tool, this code should still be a good fit.

For complete source code examples, see the https://github.com/datastax/astra-streaming-examples[{product} examples repository].

. Create a new Maven project.
+
[source,shell]
----
mvn archetype:generate \
    -DgroupId=org.example \
    -DartifactId=StarlightForKafkaClient \
    -DarchetypeArtifactId=maven-archetype-quickstart \
    -DinteractiveMode=false

cd StarlightForKafkaClient
----

. Open the new project in your IDE or text editor, and then add the Kafka client dependency to `pom.xml`:
+
[source,xml]
----
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-clients</artifactId>
  <version>3.3.1</version>
</dependency>
----

. Open the file `src/main/java/org/example/App.java`, and then enter the following code.
If you cloned the example repo, replace the entire contents of `App.java` with the following code.
Your editor will report an error because this isn't a complete script yet.
+
Replace placeholders with the values you previously retrieved from {product}.
+
[source,java]
----
package org.example;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.LongSerializer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class App {
  private static String bootstrapServers = "<REPLACE_WITH_BOOTSTRAP_SERVER_URL>";
  private static String pulsarToken = "<REPLACE_WITH_PULSAR_TOKEN>";
  private static String tenantName = "<REPLACE_WITH_TENANT_NAME>";
  private static final String namespace = "<REPLACE_WITH_NAMESPACE>";
  private static final String topicName = "<REPLACE_WITH_TOPIC>";
  private static final String topic = String.format("persistent://%s/%s/%s", tenantName,namespace,topicName);

  public static void main(String[] args) {
----

. Add the following code that builds the configuration that will be used by both the producer and consumer:
+
[source,java]
----
    Properties config = new Properties();
    config.put("bootstrap.servers",bootstrapServers);
    config.put("security.protocol","SASL_SSL");
    config.put("sasl.jaas.config", String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username='%s' password='token:%s';", tenantName, pulsarToken));
    config.put("sasl.mechanism","PLAIN");
    config.put("session.timeout.ms","45000");
    config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, LongSerializer.class.getName());
    config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put("group.id", "my-consumer-group");
----

. Add the producer code, which is a simple flow that sends a single message and awaits acknowledgment:
+
[source,java]
----
    KafkaProducer<Long, String> producer = new KafkaProducer<>(config);

    final ProducerRecord<Long, String> producerRecord = new ProducerRecord<>(topic, System.currentTimeMillis(), "Hello World");
    producer.send(producerRecord, new Callback() {
      public void onCompletion(RecordMetadata metadata, Exception e) {
        if (e != null)
          System.out.println(String.format("Send failed for record, %s. \nRecord data: %s",e.getMessage(), producerRecord));
        else
          System.out.println("Successfully sent message");
      }
    });

    producer.flush();
    producer.close();
----

. Add the consumer code, which creates a basic subscription and retrieves the latest messages on the topic:
+
[source,java]
----
    final KafkaConsumer<Integer, String> consumer = new KafkaConsumer<Integer, String>(config);

    consumer.subscribe(Collections.singletonList(topic));
    ConsumerRecords<Integer, String> consumerRecords = consumer.poll(Duration.ofMillis(5000));

    System.out.println(String.format("Found %d total record(s)", consumerRecords.count()));

    for (ConsumerRecord<Integer, String> consumerRecord : consumerRecords) {
      System.out.println(consumerRecord);
    }

    consumer.commitSync();
    consumer.close();
  }
}
----

. Build and run a JAR file for the complete program:
+
[source,shell]
----
mvn clean package assembly:single
java -jar target/StarlightForKafkaClient-1.0-SNAPSHOT-jar-with-dependencies.jar
----
+
.Result
[%collapsible]
====
[source,shell]
----
Successfully sent message

Found 1 total record(s)
ConsumerRecord(topic = persistent://my-tenant-007/my-namespace/my-topic, partition = 0, leaderEpoch = null, offset = 22, CreateTime = 1673545962124, serialized key size = 8, serialized value size = 11, headers = RecordHeaders(headers = [], isReadOnly = false), key =   xxxxx, value = Hello World)
----
====
--
======

== See also

* xref:starlight-for-kafka:operations:starlight-kafka-kstreams.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-implementation.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-monitor.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-security.adoc[]
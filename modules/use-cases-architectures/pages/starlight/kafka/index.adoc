= Get started with the {starlight-kafka} extension
:navtitle: {starlight-kafka}
:description: Use the {starlight-kafka} extension with {pulsar-reg}.

The https://github.com/datastax/starlight-for-kafka[{starlight-kafka} extension] brings the native Apache Kafka(R) protocol support to {pulsar-reg} by introducing a Kafka protocol handler on {pulsar-short} brokers.
By adding the {starlight-kafka} protocol handler to your existing {pulsar-short} cluster, you can migrate your existing Kafka applications and services to {pulsar-short} without modifying the code.

For more information about the {starlight-kafka} architecture, see the xref:starlight-for-kafka:ROOT:index.adoc[{starlight-kafka} documentation].

image:s4k-architecture.png[{starlight-kafka} Architecture]

== Establish the Kafka protocol handler

Before a Kafka client can interact with your {pulsar-short} cluster, you need the {starlight-kafka} protocol handler installed in the cluster.
Installation looks a bit different depending on where your {pulsar-short} cluster is running.
Choose the option that best fits your needs.

[tabs]
======
Astra Streaming::
+
--
Get started producing and consuming Kafka messages on a {pulsar-short} cluster.

=== Enable {starlight-kafka} on an Astra Streaming tenant

. Sign in to your Astra account, go to Astra Streaming, and then xref:astra-streaming:getting-started:index.adoc[create a tenant] or click an existing tenant.

. Go to your tenant's **Connect** tab, select **Kafka**, and then click **Enable Kafka**.

. Review the information about the {starlight-kafka} extension, and then click **Enable Kafka** to confirm that you want to enable this extension on your tenant.
+
[IMPORTANT]
====
This action creates a configuration file and the following three namespaces in your Astra Streaming tenant automatically:

* `kafka`: Produces and consumes messages
* `+__kafka+`: Supports required Kafka functionality
* `+__kafka_unlimited+`: Stores metadata

These namespaces are required for the {starlight-kafka} extension to function properly.
These are permanent namespaces that cannot be removed except by deleting the entire tenant and all of its data.
====

. Save the Kafka configuration details to a file named `ssl.properties`.
The actual values depend on your Astra Streaming tenant's configuration and cloud provider.
+
.ssl.properties
[source,plain,subs="+quotes"]
----
username: **TENANT_NAME**
password: token:***
bootstrap.servers: kafka-**PROVIDER**-**REGION**.streaming.datastax.com:9093
schema.registry.url: https://kafka-**PROVIDER**-**REGION**.streaming.datastax.com:8081
security.protocol: SASL_SSL
sasl.mechanism: PLAIN
----

=== Connect Kafka and {pulsar-short}
// Move to messaging section?
This example uses tools included with the https://kafka.apache.org/downloads[Apache Kafka tarball].

. In Astra Streaming, create a new topic in the `kafka` namespace.
+
No specific name is required.

. In your Kafka instance, move your `ssl.properties` file to your `Kafka_2.13-3.1.0/config` folder.
These values are required for SSL encryption.
The exact values depend on your Astra Streaming tenant's configuration.
+
[source,properties,subs="+quotes"]
----
bootstrap.servers=kafka-**PROVIDER**-**REGION**.streaming.datastax.com:9093
security.protocol=SASL_SSL
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='**TENANT_NAME**' password='token:{pulsar tenant token}'
sasl.mechanism=PLAIN
session.timeout.ms=45000
----

. In the `Kafka` directory, create a Kafka producer to produce messages on the topic you created in the `kafka` namespace:
+
[source,shell,subs="+quotes"]
----
bin/kafka-console-producer \
--broker-list kafka-**PROVIDER**-**REGION**.streaming.datastax.com:9093 \
--topic **TENANT_NAME**/kafka/**TOPIC_NAME** \
--producer.config config/ssl.properties
----
+
Once the producer is ready, it accepts standard input from the user:
+
[source,console]
----
>hello pulsar
----

. In a new terminal window, create a Kafka consumer to consume messages from the beginning of your topic:
+
[source,shell]
----
bin/kafka-console-consumer \
--bootstrap-server kafka-**PROVIDER**-**REGION**.streaming.datastax.com:9093 \
--topic **TENANT_NAME**/kafka/**TOPIC_NAME** \
--consumer.config config/ssl.properties \
--from-beginning
----

. Send a few messages in the terminal to generate traffic on the tenant.

. In Astra Streaming, go to your tenant's **Namespaces and Topics** tab to inspect the activity in the `kafka` namespace.
+
To verify that your Kafka messages are being produced and consumed in your Astra Streaming {pulsar-short} cluster, check the **Data In** metrics.
The number of messages should be, at least, equal to the number of messages you sent with your Kafka producer on the command line.
--

Luna Streaming::
+
--
The {starlight-kafka} extension is included in the `luna-streaming-all` image used to deploy a Luna cluster.
The Luna Helm chart simplifies deployment of the Kafka extension.

The following steps explain how to deploy a Luna Streaming Helm chart to create a simple {pulsar-short} cluster with the Starlight for Kafka protocol handler extension.

. Make sure you meet the following prerequisites:
+
* https://helm.sh/docs/intro/install/[Helm 3 CLI] (we used version 3.8.0)
* https://www.apache.org/dyn/closer.cgi?path=/kafka/3.3.1/kafka_2.13-3.3.1.tgz[Kafka CLI] (we used version 3.3.1)
* https://kubernetes.io/docs/tasks/tools/[Kubectl CLI] (we used version 1.23.4)
* Enough access to a K8s cluster to create a namespace, deployments, and pods

. Add the {company} Helm chart repo to your Helm store:
+
[source,shell]
----
helm repo add datastax-pulsar https://datastax.github.io/pulsar-helm-chart
----

. Install the Helm chart using a minimalist values file.
This command creates a Helm release named `my-pulsar-cluster` using the {company} Luna Helm chart, within the K8s namespace `datastax-pulsar`.
The minimal cluster creates only the essential components and has no ingress or load balanced services.
+
[source,shell]
----
VALUES_URL="https://raw.githubusercontent.com/datastaxdevs/luna-streaming-examples/main/starlight-for-kafka/values.yaml"
helm install \
  --namespace datastax-pulsar \
  --create-namespace \
  --values $VALUES_URL \
  --version 3.0.4 \
  my-pulsar-cluster \
  datastax-pulsar/pulsar
----

. Wait for the broker pod to reach a running state.
It might restart a few times while your components start up.
+
[source,shell]
----
kubectl -n datastax-pulsar wait --for=condition=Ready pod/pulsar-broker-0 --timeout=120s
----

. Forward service ports so you can interact with certain services on the Kubernetes cluster:
+
.. In a new terminal, port forward {pulsar-short}'s admin service:
+
[source,shell]
----
kubectl port-forward -n datastax-pulsar service/pulsar-broker 8080:8080
----

.. In a separate terminal window, port forward the Starlight for Kafka service:
+
[source,shell]
----
kubectl port-forward -n datastax-pulsar service/pulsar-proxy 9092:9092
----

. Inspect the created namespaces and topics.
+
The Luna Streaming Helm Chart automatically creates a tenant named `public` and a namespace within that tenant named `default`.
The Starlight for Kafka extension creates a few namespaces and topics automatically because these are required for the extension to function correctly.
+
List the namespaces in the `public` tenant:
+
[source,shell]
----
~/apache-pulsar-3.1.3$ ./bin/pulsar-admin namespaces list public
----
+
The output should be similar to the following:
+
[source,shell]
----
public/__kafka
public/__kafka_producerid
public/default
----
+
The automatically generated namespaces prefixed with "+__kafka+".
These are used by the service for different functions.
For more information, see the xref:starlight-for-kafka:ROOT:index.adoc[Starlight for Kafka documentation].

=== Produce a message with the Kafka CLI

//Move to messaging section

If you hadn't noticed, we never opened the {pulsar-short} binary port to accept new messages.
Only the admin port and the Kafka port are open.
To further show how native Starlight for Kafka is to {pulsar-short}, we will use the Kafka CLI to produce and consume messages from {pulsar-short}.

From within the Kafka directory, run the following command to start the shell:

[source,shell]
----
~/kafka_2.13-3.3.1$ ./bin/kafka-console-producer.sh --topic quickstart --bootstrap-server localhost:9092
----

Type a message, press Enter to send it, then Ctrl-C to exit the producer shell.

[source,shell]
----
This my first message
----

A `quickstart` topic is created automatically because the default behavior of Starlight for Kafka is to create a new single partition, persistent topic when one is not present.
You can configure this behavior and many other S4K parameters in the https://github.com/datastaxdevs/luna-streaming-examples/blob/main/starlight-for-kafka/values.yaml[Helm chart].
Learn more about the configuration values xref:starlight-for-kafka:configuration:starlight-kafka-configuration.adoc[here].

Let's have a look at the topic that was created. From your {pulsar-short} home folder, run the following command:

[source,shell]
----
~/apache-pulsar-3.1.3$ ./bin/pulsar-admin topics list public/default
----

The output will include the newly created topic:

[source,shell]
----
persistent://public/default/quickstart-partition-0
----

== Consume the new message with the Kafka CLI
//Move to messaging section
Let's use the Kafka CLI to consume the message we just produced.

Start the consumer shell from the Kafka home folder with the following command:

[source,shell]
----
~/kafka_2.13-3.3.1$ ./bin/kafka-console-consumer.sh --topic quickstart --from-beginning --bootstrap-server localhost:9092
----

The data of our new message will be output:

[source,shell]
----
This my first message
----

Enter Ctrl-C to exit the shell.
--

Self Managed::
+
--
Already have your own {pulsar-short} cluster? Or maybe you're using a standalone cluster? {starlight-kafka} can easily be a part of that cluster! Follow the "xref:starlight-for-kafka:installation:starlight-kafka-quickstart.adoc[]" guide.
--
======

== Message with {starlight-kafka}

{starlight-kafka} supports quite a few different use cases. With a {pulsar-short} cluster between producers and consumers you can interchange the type of producer and consumer to fit your needs.

//Use tabs instead of astra only. Compare with what is already in the target Starlight topics.
[TIP]
====
The following examples use an Astra Streaming tenant as the Kafka bootstrap server.

If you are using Luna Streaming or a self-managed tenant, use the bootstrap server URL for your tenant.
====

=== Retrieve Kafka connection properties in Astra Streaming

//Already covered on the Astra tab above. Image isn't needed.
//See if this can be made generic or already provided for both in the Starlight docs

In the Astra Streaming portal "Connect" tab, the "kafka" area provides important connection information.
You will need this connection information to create a working Kafka client or use the CLI.

image:kafka-client-settings.png[Astra Streaming kafka settings]

TIP: Click the clipboard icon to copy the Kafka connection values, as well as a working token to paste in code.

=== Produce and consume a message

[tabs]
======
Kafka CLI::
+
--
Download the latest Kafka distribution https://www.apache.org/dyn/closer.cgi?path=/kafka/3.3.1/kafka_2.13-3.3.1.tgz[here].
With the tarball extracted, the producer and consumer CLIs are in the 'bin' folder.

. To get started, let's set a few variables. If you've completed our "xref:astra-streaming:getting-started:index.adoc[Getting started with Astra Streaming]" guide, the below values will be a perfect fit for your existing tenant.
+
[source,shell]
----
SERVICE_URL="<REPLACE_WITH_BOOTSTRAP_SERVER_URL>"
TENANT="<REPLACE_WITH_TENANT_NAME>"
NAMESPACE="<REPLACE_WITH_NAMESPACE>"
TOPIC="<REPLACE_WITH_TOPIC>"
----

. Now let's enter those variables in Kafka's producer shell.
+
[source,shell]
----
# cd kafka_2.13-3.3.1
./bin/kafka-console-producer.sh --topic "$TENANT/$NAMESPACE/$TOPIC" --bootstrap-server "$SERVICE_URL"
----

. Type in a super memorable message and hit 'enter' to send. Press 'Ctrl-C' to exit the shell.
+
[source,shell]
----
> This is my first S4K message.
----
A new message has been produced in the provided tenant/namespace/topic and is ready for consumption.
. Start the Kafka consumer shell.
+
[source,shell]
----
# cd kafka_2.13-3.3.1
./bin/kafka-console-consumer.sh --topic "$TENANT/$NAMESPACE/$TOPIC" --from-beginning --bootstrap-server "$SERVICE_URL"
----

. The consumer should immediately find the new message and output its value.
+
[source,shell]
----
This is my first S4K message.
----

. Press 'Ctrl-C' to exit the consumer shell.
--

Kafka Client (Java)::
+
--
This example uses Maven for the project structure.
If you prefer Gradle or another tool, this code should still be a good fit.

For complete source code examples, see the https://github.com/datastax/astra-streaming-examples[Astra Streaming examples repository].

. Create a new Maven project.
+
[source,shell]
----
mvn archetype:generate \
    -DgroupId=org.example \
    -DartifactId=StarlightForKafkaClient \
    -DarchetypeArtifactId=maven-archetype-quickstart \
    -DinteractiveMode=false

cd StarlightForKafkaClient
----

. Open the new project in your IDE or text editor, and then add the Kafka client dependency to `pom.xml`:
+
[source,xml]
----
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-clients</artifactId>
  <version>3.3.1</version>
</dependency>
----

. Open the file `src/main/java/org/example/App.java`, and then enter the following code.
If you cloned the example repo, replace the entire contents of `App.java` with the following code.
Your editor will report an error because this isn't a complete script yet.
+
Replace placeholders with the values you previously retrieved from Astra Streaming.
+
[source,java]
----
package org.example;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.LongSerializer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class App {
  private static String bootstrapServers = "<REPLACE_WITH_BOOTSTRAP_SERVER_URL>";
  private static String pulsarToken = "<REPLACE_WITH_PULSAR_TOKEN>";
  private static String tenantName = "<REPLACE_WITH_TENANT_NAME>";
  private static final String namespace = "<REPLACE_WITH_NAMESPACE>";
  private static final String topicName = "<REPLACE_WITH_TOPIC>";
  private static final String topic = String.format("persistent://%s/%s/%s", tenantName,namespace,topicName);

  public static void main(String[] args) {
----

. Add the following code that builds the configuration that will be used by both the producer and consumer:
+
[source,java]
----
    Properties config = new Properties();
    config.put("bootstrap.servers",bootstrapServers);
    config.put("security.protocol","SASL_SSL");
    config.put("sasl.jaas.config", String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username='%s' password='token:%s';", tenantName, pulsarToken));
    config.put("sasl.mechanism","PLAIN");
    config.put("session.timeout.ms","45000");
    config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, LongSerializer.class.getName());
    config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put("group.id", "my-consumer-group");
----

. Add the producer code, which is a simple flow that sends a single message and awaits acknowledgment:
+
[source,java]
----
    KafkaProducer<Long, String> producer = new KafkaProducer<>(config);

    final ProducerRecord<Long, String> producerRecord = new ProducerRecord<>(topic, System.currentTimeMillis(), "Hello World");
    producer.send(producerRecord, new Callback() {
      public void onCompletion(RecordMetadata metadata, Exception e) {
        if (e != null)
          System.out.println(String.format("Send failed for record, %s. \nRecord data: %s",e.getMessage(), producerRecord));
        else
          System.out.println("Successfully sent message");
      }
    });

    producer.flush();
    producer.close();
----

. Add the consumer code, which creates a basic subscription and retrieves the latest messages on the topic:
+
[source,java]
----
    final KafkaConsumer<Integer, String> consumer = new KafkaConsumer<Integer, String>(config);

    consumer.subscribe(Collections.singletonList(topic));
    ConsumerRecords<Integer, String> consumerRecords = consumer.poll(Duration.ofMillis(5000));

    System.out.println(String.format("Found %d total record(s)", consumerRecords.count()));

    for (ConsumerRecord<Integer, String> consumerRecord : consumerRecords) {
      System.out.println(consumerRecord);
    }

    consumer.commitSync();
    consumer.close();
  }
}
----

. Build and run a JAR file for the complete program:
+
[source,shell]
----
mvn clean package assembly:single
java -jar target/StarlightForKafkaClient-1.0-SNAPSHOT-jar-with-dependencies.jar
----
+
.Result
[%collapsible]
====
[source,shell]
----
Successfully sent message

Found 1 total record(s)
ConsumerRecord(topic = persistent://my-tenant-007/my-namespace/my-topic, partition = 0, leaderEpoch = null, offset = 22, CreateTime = 1673545962124, serialized key size = 8, serialized value size = 11, headers = RecordHeaders(headers = [], isReadOnly = false), key =   xxxxx, value = Hello World)
----
====
--
======

== See also

* xref:starlight-for-kafka:operations:starlight-kafka-kstreams.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-implementation.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-monitor.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-security.adoc[]
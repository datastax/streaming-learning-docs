= Get started with the {starlight-kafka} extension
:navtitle: {starlight-kafka}
:description: Use the {starlight-kafka} extension with {pulsar-reg}.

The https://github.com/datastax/starlight-for-kafka[{starlight-kafka} extension] brings the native Apache Kafka(R) protocol support to {pulsar-reg} by introducing a Kafka protocol handler on {pulsar-short} brokers.

By adding the {starlight-kafka} protocol handler to your existing {pulsar-short} cluster, you can migrate your existing Kafka applications and services to {pulsar-short} without modifying the code.

//Move this image to the Starlight for Kafka main index page
image:s4k-architecture.png[{starlight-kafka} Architecture]

[#establish-the-kafka-protocol-handler]
== Establish the Kafka protocol handler

Before a Kafka client can interact with your {pulsar-short} cluster, you need the {starlight-kafka} protocol handler installed in the cluster.

[tabs]
======
Astra Streaming::
+
--
These steps explain how to enable the {starlight-kafka} protocol handler in an Astra Streaming {pulsar-short} cluster, and then get the connection details for your Kafka client:

. In the {astra-ui-link} header, click icon:grip[name="Applications"], and then select *Streaming*.

. xref:astra-streaming:getting-started:index.adoc[Create a tenant] or click the name of an existing tenant.

. Go to your tenant's **Connect** tab, select **Kafka**, and then click **Enable Kafka**.

. Review the information about the {starlight-kafka} extension, and then click **Enable Kafka** to confirm that you want to enable this extension on your tenant.
+
[IMPORTANT]
====
This action creates a configuration file and the following three namespaces in your Astra Streaming tenant automatically:

* `kafka`: Produces and consumes messages
* `+__kafka+`: Supports required Kafka functionality
* `+__kafka_unlimited+`: Stores metadata

These namespaces are required for the {starlight-kafka} extension to function properly.
These are permanent namespaces that cannot be removed except by deleting the entire tenant and all of its data.
====

. Save the Kafka connection details to a file named `ssl.properties`.
The actual values depend on your Astra Streaming tenant's configuration and cloud provider.
+
.ssl.properties
[source,plain,subs="+quotes,+attributes"]
----
username: **TENANT_NAME**
password: token:{asterisk}{asterisk}{asterisk}{asterisk}{asterisk}{asterisk}
bootstrap.servers: kafka-**PROVIDER**-**REGION**.streaming.datastax.com:9093
schema.registry.url: https://kafka-**PROVIDER**-**REGION**.streaming.datastax.com:8081
security.protocol: SASL_SSL
sasl.mechanism: PLAIN
----
+
[TIP]
====
In Astra Streaming, if you click icon:download[name="Download"] or icon:clipboard[name="Copy"], then a {pulsar-short} token is included in the `password` field automatically.
You can also generate a token on your tenant's **Settings** tab.
====

. Optional: Add a session timeout value to the `ssl.properties` file to avoid connection issues with some Kafka clients:
+
[source,properties]
----
session.timeout.ms=45000
----
+
Continue to the next section to learn how to use the connection details in a Kafka client script.

--

Luna Streaming::
+
--
The {starlight-kafka} extension is included in the `luna-streaming-all` image used to deploy a Luna cluster.
The Luna Helm chart simplifies deployment of the Kafka extension.

The following steps explain how to deploy a Luna Streaming Helm chart to create a simple {pulsar-short} cluster with the {starlight-kafka} extension ready to use.

. Make sure you meet the following prerequisites:
+
* Install https://helm.sh/docs/intro/install/[Helm 3 CLI] version 3.8.0 or later.
* Install https://www.apache.org/dyn/closer.cgi?path=/kafka/3.3.1/kafka_2.13-3.3.1.tgz[Kafka CLI] version 3.3.1 or later
* Install https://kubernetes.io/docs/tasks/tools/[kubectl CLI] version 1.23.4 or later.
* Have access to a Kubernetes cluster with permission to create a namespace, deployments, and pods.

. Add the {company} Helm chart repo to your Helm store:
+
[source,shell]
----
helm repo add datastax-pulsar https://datastax.github.io/pulsar-helm-chart
----

. Install the Helm chart using a minimal values file.
The following command creates a Helm release named `my-pulsar-cluster` using the {company} Luna Helm chart within a Kubernetes namespace named `datastax-pulsar`.
This minimal configuration creates only the essential components and has no ingress or load balanced services.
+
[source,shell]
----
VALUES_URL="https://raw.githubusercontent.com/datastaxdevs/luna-streaming-examples/main/starlight-for-kafka/values.yaml"
helm install \
  --namespace datastax-pulsar \
  --create-namespace \
  --values $VALUES_URL \
  --version 3.0.4 \
  my-pulsar-cluster \
  datastax-pulsar/pulsar
----

. Wait for the broker pod to reach a running state.
It might restart a few times while the components start up.
+
[source,shell]
----
kubectl -n datastax-pulsar wait --for=condition=Ready pod/pulsar-broker-0 --timeout=120s
----

. Use the {pulsar-short} Admin CLI to inspect the tenants, namespaces, and topics that were created in your {pulsar-short} cluster when you deployed the Helm chart with the {starlight-kafka} extension enabled.
+
By default, the Helm chart create a tenant named `public` with a namespace named `default`.
In addition, the {starlight-kafka} extension creates several namespaces and topics that are required for its functionality.
These namespaces are prefixed with `+__kafka+`.
+
List the namespaces in the `public` tenant:
+
[source,shell]
----
~/apache-pulsar-3.1.3$ ./bin/pulsar-admin namespaces list public
----
+
Make sure the output includes the `default` and `+__kafka+` namespaces:
+
[source,shell]
----
public/__kafka
public/__kafka_producerid
public/default
----

. Enable port forwarding for the {pulsar-short} Admin and {starlight-kafka} services that are running on the Kubernetes cluster:
+
.. In a new terminal, port forward the {pulsar-short} Admin service:
+
[source,shell]
----
kubectl port-forward -n datastax-pulsar service/pulsar-broker 8080:8080
----

.. In a separate terminal window, port forward the {starlight-kafka} service:
+
[source,shell]
----
kubectl port-forward -n datastax-pulsar service/pulsar-proxy 9092:9092
----

+
[TIP]
====
You don't need to open the {pulsar-short} binary port to accept new messages when using {starlight-kafka}.
This is because Kafka clients communicate using the Kafka protocol on port 9092, which is handled by the {starlight-kafka} extension.
====
After deploying the Helm chart and enabling port forwarding, your applications can communicate with {pulsar-short} as if it were a real Kafka host, as explained in the next section.

--

Self-managed::
+
--
To use {starlight-kafka} with self-managed or standalone {pulsar-short} clusters, see xref:starlight-for-kafka:installation:starlight-kafka-quickstart.adoc[].
--
======

== Produce and consume messages with {starlight-kafka}

This section explains how to use your Pulsar tenant's connection details with a Kafka client to produce and consume messages with {starlight-kafka}.

{starlight-kafka} supports many different use cases.
With a {pulsar-short} cluster between publishers and consumers, you can change the type of publisher and consumer to fit your needs.

The following examples use the Kafka CLI and Java.
For complete source code examples and examples for other languages, see the https://github.com/datastax/astra-streaming-examples[{company} streaming examples repository].
These examples are written for Astra Streaming but they can be adapted for Luna Streaming or self-managed {pulsar-short} clusters by replacing the connection details with those for your cluster.

[tabs]
======
Astra Streaming::
+
--
[tabs]
====
Kafka CLI::
+
This example uses command line tools included with the https://kafka.apache.org/downloads[Apache Kafka tarball].
+
. Enable {starlight-kafka} and get the `ssl.properties` connection details, as explained in <<establish-the-kafka-protocol-handler>>.
+
. In Astra Streaming, in the `kafka` namespace, create a topic to receive messages from the `kafka-console-producer`.
You can use any valid name for the topic.
+
. Move your Astra Streaming `ssl.properties` file into your Kafka installation's `config` subdirectory, such as `kafka_2.13-3.1.0/config`.
+
. In a terminal window, use the `kafka-console-producer` tool to create a Kafka producer that produces messages on the topic you created in the `kafka` namespace:
+
[source,shell,subs="+quotes"]
----
bin/kafka-console-producer \
--broker-list kafka-**PROVIDER**-**REGION**.streaming.datastax.com:9093 \
--topic **TENANT_NAME**/kafka/**TOPIC_NAME** \
--producer.config config/ssl.properties
----
+
When the producer is ready, it accepts standard input from the user.
To send a message, type it in the terminal, and then press kbd:[Return].
Optionally, you can send some messages before starting the consumer to see how it retrieves past messages.
+
. In a new terminal window, use the `kafka-console-consumer` tool to create a Kafka consumer that consumes messages from the beginning of your topic:
+
[source,shell,subs="+quotes"]
----
bin/kafka-console-consumer \
--bootstrap-server kafka-**PROVIDER**-**REGION**.streaming.datastax.com:9093 \
--topic **TENANT_NAME**/kafka/**TOPIC_NAME** \
--consumer.config config/ssl.properties \
--from-beginning
----
+
Because this command uses the `--from-beginning` option, when the consumer starts, it retrieves any messages that were sent to the topic before it started.
+
. Send a few messages in the `kafka-console-producer` terminal to generate more traffic on the tenant.
+
As you send messages and the consumer receives them, they appear in the `kafka-console-consumer` terminal.
+
. In Astra Streaming, go to your tenant's **Namespaces and Topics** tab to inspect the activity in the `kafka` namespace.
To verify that your Kafka messages are being produced and consumed in your Astra Streaming {pulsar-short} cluster, check the **Data In** metrics.
The number of messages should be equal to the number of messages you sent with the `kafka-console-producer`.
+
. To exit the producer and consumer shells, press kbd:[Ctrl]+kbd:[C] in each terminal.

Java client::
+
The following example uses a Java program to create a connection between Kafka and your Astra Streaming tenant, configures a producer and consumer, and then sends a message.
This example uses Maven, but you can also use Gradle.
+
. Enable {starlight-kafka} and get the `ssl.properties` connection details, as explained in <<establish-the-kafka-protocol-handler>>.
+
. Create a new Maven project:
+
[source,shell]
----
mvn archetype:generate \
    -DgroupId=org.example \
    -DartifactId=StarlightForKafkaClient \
    -DarchetypeArtifactId=maven-archetype-quickstart \
    -DinteractiveMode=false
----
+
. Change to the new project directory:
+
[source,shell]
----
cd StarlightForKafkaClient
----
+
. Open the new project in your IDE, and then add the Kafka client dependency to `pom.xml`:
+
.pom.xml
[source,xml]
----
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-clients</artifactId>
  <version>3.3.1</version>
</dependency>
----
+
. Open the `App.java` file at `src/main/java/org/example/App.java`, and then delete any preexisting code in this file.
In the next steps, you will add code to this file to create a complete program that produces and consumes messages.
+
. Paste the following code in the file, and then replace the placeholder values with the values from your `ssl.properties` file.
Your editor will report errors because this isn't a complete program yet.
+
[source,java,subs="+quotes"]
----
package org.example;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.LongSerializer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class App {
  private static String bootstrapServers = "**BOOTSTRAP_SERVER_URL**";
  private static String pulsarToken = "**PASSWORD**";
  private static String tenantName = "**USERNAME**";
  private static final String namespace = "kafka";
  private static final String topicName = "quickstart";
  private static final String topic = String.format("persistent://%s/%s/%s", tenantName,namespace,topicName);

  public static void main(String[] args) {
----
+
. Optional: Replace `quickstart` with another name for the topic that receives messages from the Kafka producer.
If the topic doesn't exist, it is created it automatically when the producer sends the first message.
+
. Add the following code that builds the configuration for the producer and consumer:
+
[source,java]
----
    Properties config = new Properties();
    config.put("bootstrap.servers",bootstrapServers);
    config.put("security.protocol","SASL_SSL");
    config.put("sasl.jaas.config", String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username='%s' password='token:%s';", tenantName, pulsarToken));
    config.put("sasl.mechanism","PLAIN");
    config.put("session.timeout.ms","45000");
    config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, LongSerializer.class.getName());
    config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put("group.id", "my-consumer-group");
----
+
. Add the producer code, which is a simple flow that sends a single message and awaits acknowledgment:
+
[source,java]
----
    KafkaProducer<Long, String> producer = new KafkaProducer<>(config);

    final ProducerRecord<Long, String> producerRecord = new ProducerRecord<>(topic, System.currentTimeMillis(), "Hello World");
    producer.send(producerRecord, new Callback() {
      public void onCompletion(RecordMetadata metadata, Exception e) {
        if (e != null)
          System.out.println(String.format("Send failed for record, %s. \nRecord data: %s",e.getMessage(), producerRecord));
        else
          System.out.println("Successfully sent message");
      }
    });

    producer.flush();
    producer.close();
----
+
. Add the consumer code, which creates a basic subscription and retrieves the latest messages on the topic:
+
[source,java]
----
    final KafkaConsumer<Integer, String> consumer = new KafkaConsumer<Integer, String>(config);

    consumer.subscribe(Collections.singletonList(topic));
    ConsumerRecords<Integer, String> consumerRecords = consumer.poll(Duration.ofMillis(5000));

    System.out.println(String.format("Found %d total record(s)", consumerRecords.count()));

    for (ConsumerRecord<Integer, String> consumerRecord : consumerRecords) {
      System.out.println(consumerRecord);
    }

    consumer.commitSync();
    consumer.close();
  }
}
----
+
. Save `App.java`, and then build and run the JAR file for the complete program:
+
[source,shell]
----
mvn clean package assembly:single
java -jar target/StarlightForKafkaClient-1.0-SNAPSHOT-jar-with-dependencies.jar
----
+
. Make sure the result shows that a message was produces and consumed:
+
[source,shell]
----
Successfully sent message

Found 1 total record(s)
ConsumerRecord(topic = persistent://my-tenant-007/my-namespace/my-topic, partition = 0, leaderEpoch = null, offset = 22, CreateTime = 1673545962124, serialized key size = 8, serialized value size = 11, headers = RecordHeaders(headers = [], isReadOnly = false), key =   xxxxx, value = Hello World)
----
+
. In Astra Streaming, go to your tenant's **Namespaces and Topics** tab to inspect the activity in the `kafka` namespace.
+
If you everything was configured correctly, then the `kafka` namespace should have a topic named `quickstart` that was created by the Java program.
Additionally, the namespace's metrics should reflect that at least one message was published and consumed by your Astra Streaming {pulsar-short} topic.
====

--

Luna Streaming::
+
--
To use the Kafka CLI or a Kafka client with {starlight-kafka}, you use your Luna Streaming {pulsar-short} tenant as the Kafka bootstrap server.
The following example command line tools included with the https://kafka.apache.org/downloads[Apache Kafka tarball], and it connects on `localhost:9092` because port forwarding was enabled in <<establish-the-kafka-protocol-handler>>.

. In your Kafka installation directory, start the `kafka-console-producer` shell:
+
[source,shell]
----
bin/kafka-console-producer.sh \
--topic quickstart \
--bootstrap-server localhost:9092
----
+
You can replace `quickstart` with any other name for the topic that will receive messages from the producer.
If the specified topic doesn't exist in your Luna Streaming tenant, it is created automatically when the producer sends the first message.
You can configure this behavior and other xref:starlight-for-kafka:configuration:starlight-kafka-configuration.adoc[{starlight-kafka} parameters] in the https://github.com/datastaxdevs/luna-streaming-examples/blob/main/starlight-for-kafka/values.yaml[Helm chart].
+
.Connect to a remote cluster
[%collapsible]
====
To connect to a remote Luna Streaming cluster, provide the full topic name and the bootstrap server URL for your Luna Streaming tenant:

[source,shell,subs="+quotes"]
----
bin/kafka-console-producer.sh --topic "**TENANT_NAME**/**NAMESPACE_NAME**/**TOPIC_NAME**" --bootstrap-server "**SERVICE_URL**"
----
====

. When the producer is ready, type a message, and then press kbd:[Return] to send it.
Optionally, you can send several messages before starting the consumer to see how it retrieves past messages.

. In a new terminal window, start the `kafka-console-consumer` shell:
+
[source,shell]
----
bin/kafka-console-consumer.sh \
--topic quickstart \
--bootstrap-server localhost:9092 \
--from-beginning
----
+
Because this command uses the `--from-beginning` option, when the consumer starts, it retrieves any messages that were sent to the topic before it started.

. Send a few messages in the `kafka-console-producer` terminal to generate more traffic on the tenant.
+
As you send messages and the consumer receives them, they appear in the `kafka-console-consumer` terminal.

. Use the {pulsar-short} Admin CLI or the Luna Streaming {pulsar-short} Admin Console to inspect your tenant's activity, and verify that messages were published and consumed on the specified topic.

. To exit the producer and consumer shells, press kbd:[Ctrl]+kbd:[C] in each terminal.
--

Self-managed::
+
--
To use the Kafka CLI or a Kafka client with {starlight-kafka}, you use your {pulsar-short} tenant as the Kafka bootstrap server.
The following example command line tools included with the https://kafka.apache.org/downloads[Apache Kafka tarball], and it assumes port forwarding is enabled to connect on `localhost:9092`.

. In your Kafka installation directory, start the `kafka-console-producer` shell:
+
[source,shell]
----
bin/kafka-console-producer.sh \
--topic quickstart \
--bootstrap-server localhost:9092
----
+
You can replace `quickstart` with any other name for the topic that will receive messages from the producer.
If the specified topic doesn't exist in your {pulsar-short} tenant, it is created automatically when the producer sends the first message.
You can configure this behavior and other xref:starlight-for-kafka:configuration:starlight-kafka-configuration.adoc[{starlight-kafka} parameters].
+
.Connect to a remote cluster
[%collapsible]
====
To connect to a remote {pulsar-short} cluster, provide the full topic name and the bootstrap server URL for your tenant:

[source,shell,subs="+quotes"]
----
bin/kafka-console-producer.sh --topic "**TENANT_NAME**/**NAMESPACE_NAME**/**TOPIC_NAME**" --bootstrap-server "**SERVICE_URL**"
----
====

. When the producer is ready, type a message, and then press kbd:[Return] to send it.
Optionally, you can send several messages before starting the consumer to see how it retrieves past messages.

. In a new terminal window, start the `kafka-console-consumer` shell:
+
[source,shell]
----
bin/kafka-console-consumer.sh \
--topic quickstart \
--bootstrap-server localhost:9092 \
--from-beginning
----
+
Because this command uses the `--from-beginning` option, when the consumer starts, it retrieves any messages that were sent to the topic before it started.

. Send a few messages in the `kafka-console-producer` terminal to generate more traffic on the tenant.
+
As you send messages and the consumer receives them, they appear in the `kafka-console-consumer` terminal.

. Use the {pulsar-short} Admin CLI to inspect your tenant's activity, and verify that messages were published and consumed on the specified topic.

. To exit the producer and consumer shells, press kbd:[Ctrl]+kbd:[C] in each terminal.
--
======

== See also

* xref:starlight-for-kafka:operations:starlight-kafka-kstreams.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-implementation.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-monitor.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-security.adoc[]
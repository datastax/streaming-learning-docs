= Getting started with the {kafka-for-astra} extension
:navtitle: {kafka-for-astra}
:description: Learn how to get started using the {kafka-for-astra} extension with {pulsar-reg} and get hands on with Kafka producer and consumer interacting with a topic.

{kafka-for-astra} brings the native Apache Kafka(R) protocol support to {pulsar-reg} by introducing a Kafka protocol handler on {pulsar-short} brokers. By adding the {kafka-for-astra} protocol handler to your existing {pulsar-short} cluster, you can migrate your existing Kafka applications and services to {pulsar-short} without modifying the code.

If source code is your thing, visit the https://github.com/datastax/starlight-for-kafka[project's repo on GitHub].

== Architecture reference

If you would like to get deep into how {kafka-for-astra} works, xref:starlight-for-kafka:ROOT:index.adoc[read the docs].

image:s4k-architecture.png[{kafka-for-astra} Architecture]

== Establishing the Kafka protocol handler

Before a Kafka client can interact with your {pulsar-short} cluster, you need the {kafka-for-astra} protocol handler installed in the cluster.
Installation looks a bit different depending on where your {pulsar-short} cluster is running.
Choose the option that best fits your needs.

[tabs]
====
Astra Streaming::
+
--

If you want a working Kafka extension as quickly as possible, this is your best bet.
This is also a good option for those that already have a streaming tenant and are looking to extend it.

. Sign in to your Astra account and navigate to your streaming tenant.
+
TIP: Don't have a streaming tenant? Follow our "xref:astra-streaming:getting-started:index.adoc[]" guide.

. Go to the "Connect" tab and choose the "Kafka" option.

. Click "Enable Kafka".

. A message will let you know of the additions (and restrictions) that come with using {kafka-for-astra}.

. Select the "Enable Kafka" button to confirm your understanding.

Your Astra Streaming tenant is ready for prime time! Continue to the next section of the guide to see it in action.
--
Luna Streaming::
+
--
The {kafka-for-astra} extension is included in the `luna-streaming-all` image used to deploy a Luna cluster. The Luna helm chart makes deploying the Kafka extension quite easy. Follow the "xref:luna-streaming:components:starlight-for-kafka.adoc[]" guide to create a simple {pulsar-short} cluster with the {kafka-for-astra} extension ready for use.

== Use Starlight for Kafka with Luna Streaming

Starlight for Kafka brings the native Apache Kafka protocol support to {pulsar} by introducing a Kafka protocol handler on {pulsar-short} brokers.
By adding the Starlight for Kafka protocol handler to your {pulsar-short} cluster, you can migrate your existing Kafka applications and services to {pulsar-short} without modifying the code.

The following steps explain how to deploy a Luna Streaming Helm chart with the Starlight for Kafka protocol handler extension.

=== Prerequisites

* https://helm.sh/docs/intro/install/[Helm 3 CLI] (we used version 3.8.0)
* https://www.apache.org/dyn/closer.cgi?path=/kafka/3.3.1/kafka_2.13-3.3.1.tgz[Kafka CLI] (we used version 3.3.1)
* https://kubernetes.io/docs/tasks/tools/[Kubectl CLI] (we used version 1.23.4)
* Enough access to a K8s cluster to create a namespace, deployments, and pods

=== Install Luna Streaming Helm chart

. Add the {company} Helm chart repo to your Helm store.
+
[source,shell]
----
helm repo add datastax-pulsar https://datastax.github.io/pulsar-helm-chart
----

. Install the Helm chart using a minimalist values file.
This command creates a Helm release named `my-pulsar-cluster` using the {company} Luna Helm chart, within the K8s namespace `datastax-pulsar`.
The minimal cluster creates only the essential components and has no ingress or load balanced services.
+
[source,shell]
----
VALUES_URL="https://raw.githubusercontent.com/datastaxdevs/luna-streaming-examples/main/starlight-for-kafka/values.yaml"
helm install \
  --namespace datastax-pulsar \
  --create-namespace \
  --values $VALUES_URL \
  --version 3.0.4 \
  my-pulsar-cluster \
  datastax-pulsar/pulsar
----

. Wait for the broker pod to be in a running state. You might see a few restarts as your components start up.
+
[source,shell]
----
kubectl -n datastax-pulsar wait --for=condition=Ready pod/pulsar-broker-0 --timeout=120s
----

=== Forward service port

You'll need to interact with a few of the services in the K8s cluster.
Map a few ports to those services.

In a new terminal, port forward {pulsar-short}'s admin service:

[source,shell]
----
kubectl port-forward -n datastax-pulsar service/pulsar-broker 8080:8080
----

In a separate terminal window, port forward the Starlight for Kafka serivce:

[source,shell]
----
kubectl port-forward -n datastax-pulsar service/pulsar-proxy 9092:9092
----

=== Inspect the created namespaces and topics

The Luna Streaming Helm Chart automatically creates a tenant named "public" and a namespace within that tenant named "default".

The Starlight for Kafka extension creates a few namespaces and topics to function correctly.

List the namespaces in the "public" tenant to see what was created:

[source,shell]
----
~/apache-pulsar-2.10.1$ ./bin/pulsar-admin namespaces list public
----

The output should be similar to the following.

[source,shell]
----
public/__kafka
public/__kafka_producerid
public/default
----

Notice the namespaces prefixed with "__kafka".
These are used by the service for different functions.
To learn more about Starlight for Kafka operations, see the S4K xref:starlight-for-kafka:ROOT:index.adoc[documentation].

=== Produce a message with the Kafka CLI

If you hadn't noticed, we never opened the {pulsar-short} binary port to accept new messages.
Only the admin port and the Kafka port are open.
To further show how native Starlight for Kafka is to {pulsar-short}, we will use the Kafka CLI to produce and consume messages from {pulsar-short}.

From within the Kafka directory, run the following command to start the shell:

[source,shell]
----
~/kafka_2.13-3.3.1$ ./bin/kafka-console-producer.sh --topic quickstart --bootstrap-server localhost:9092
----

Type a message, press Enter to send it, then Ctrl-C to exit the producer shell.

[source,shell]
----
This my first message
----

Wait a second! We never created a topic! And where did the "quickstart" topic come from?!

The default behavior of Starlight for Kafka is to create a new single partition, persistent topic when one is not present.
You can configure this behavior and many other S4K parameters in the https://github.com/datastaxdevs/luna-streaming-examples/blob/main/starlight-for-kafka/values.yaml[Helm chart].
Learn more about the configuration values xref:starlight-for-kafka:configuration:starlight-kafka-configuration.adoc[here].

Let's have a look at the topic that was created. From your {pulsar-short} home folder, run the following command:

[source,shell]
----
~/apache-pulsar-2.10.1$ ./bin/pulsar-admin topics list public/default
----

The output will include the newly created topic:

[source,shell]
----
persistent://public/default/quickstart-partition-0
----

== Consume the new message with the Kafka CLI

Let's use the Kafka CLI to consume the message we just produced.

Start the consumer shell from the Kafka home folder with the following command:

[source,shell]
----
~/kafka_2.13-3.3.1$ ./bin/kafka-console-consumer.sh --topic quickstart --from-beginning --bootstrap-server localhost:9092
----

The data of our new message will be output. Enter Ctrl-C to exit the shell.

[source,shell]
----
This my first message
----

=== Next steps

Kafka users and existing applications using Kafka can enjoy the many benefits of a {pulsar-short} cluster, while never having to change tooling or libraries.
Other folks that are more comfortable with {pulsar-short} tooling and clients can also interact with the same topics. Together, new and legacy applications work together to create modern solutions.

Here are links to other guides and resource you might be interested in.

* xref:streaming-learning:use-cases-architectures:starlight/kafka/index.adoc[Messaging with Starlight for Kafka]
--
Self Managed::
+
--
Already have your own {pulsar-short} cluster? Or maybe you're using a standalone cluster? {kafka-for-astra} can easily be a part of that cluster! Follow the "xref:starlight-for-kafka:installation:starlight-kafka-quickstart.adoc[]" guide.
--
====

== Messaging with {kafka-for-astra}

{kafka-for-astra} supports quite a few different use cases. With a {pulsar-short} cluster between producers and consumers you can interchange the type of producer and consumer to fit your needs.

*The below examples are using an Astra Streaming tenant as the Kafka bootstrap server.* If you are using Luna Streaming or a self-managed tenant, switch the bootstrap server URL for your own.

=== Retrieve Kafka connection properties in Astra Streaming

In the Astra Streaming portal "Connect" tab, the "kafka" area provides important connection information.
You will need this connection information to create a working Kafka client or use the CLI.

image:kafka-client-settings.png[Astra Streaming kafka settings]

TIP: Click the clipboard icon to copy the Kafka connection values, as well as a working token to paste in code.

=== Produce and consume a message

[tabs]
======
Kafka CLI::
+
--
Download the latest Kafka distribution https://www.apache.org/dyn/closer.cgi?path=/kafka/3.3.1/kafka_2.13-3.3.1.tgz[here].
With the tarball extracted, the producer and consumer CLIs are in the 'bin' folder.

. To get started, let's set a few variables. If you've completed our "xref:astra-streaming:getting-started:index.adoc[Getting started with Astra Streaming]" guide, the below values will be a perfect fit for your existing tenant.
+
[source,shell]
----
SERVICE_URL="<REPLACE_WITH_BOOTSTRAP_SERVER_URL>"
TENANT="<REPLACE_WITH_TENANT_NAME>"
NAMESPACE="<REPLACE_WITH_NAMESPACE>"
TOPIC="<REPLACE_WITH_TOPIC>"
----

. Now let's enter those variables in Kafka's producer shell.
+
[source,shell]
----
# cd kafka_2.13-3.3.1
./bin/kafka-console-producer.sh --topic "$TENANT/$NAMESPACE/$TOPIC" --bootstrap-server "$SERVICE_URL"
----

. Type in a super memorable message and hit 'enter' to send. Press 'Ctrl-C' to exit the shell.
+
[source,shell]
----
> This is my first S4K message.
----
A new message has been produced in the provided tenant/namespace/topic and is ready for consumption.
. Start the Kafka consumer shell.
+
[source,shell]
----
# cd kafka_2.13-3.3.1
./bin/kafka-console-consumer.sh --topic "$TENANT/$NAMESPACE/$TOPIC" --from-beginning --bootstrap-server "$SERVICE_URL"
----

. The consumer should immediately find the new message and output its value.
+
[source,shell]
----
This is my first S4K message.
----

. Press 'Ctrl-C' to exit the consumer shell.
--

Kafka Client (Java)::
+
--
This example uses Maven for the project structure.
If you prefer Gradle or another tool, this code should still be a good fit.

For complete source code examples, see the https://github.com/datastax/astra-streaming-examples[Astra Streaming examples repository].

. Create a new Maven project.
+
[source,shell]
----
mvn archetype:generate \
    -DgroupId=org.example \
    -DartifactId=StarlightForKafkaClient \
    -DarchetypeArtifactId=maven-archetype-quickstart \
    -DinteractiveMode=false

cd StarlightForKafkaClient
----

. Open the new project in your IDE or text editor, and then add the Kafka client dependency to `pom.xml`:
+
[source,xml]
----
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-clients</artifactId>
  <version>3.3.1</version>
</dependency>
----

. Open the file `src/main/java/org/example/App.java`, and then enter the following code.
If you cloned the example repo, replace the entire contents of `App.java` with the following code.
Your editor will report an error because this isn't a complete script yet.
+
Replace placeholders with the values you previously retrieved from Astra Streaming.
+
[source,java]
----
package org.example;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.LongSerializer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class App {
  private static String bootstrapServers = "<REPLACE_WITH_BOOTSTRAP_SERVER_URL>";
  private static String pulsarToken = "<REPLACE_WITH_PULSAR_TOKEN>";
  private static String tenantName = "<REPLACE_WITH_TENANT_NAME>";
  private static final String namespace = "<REPLACE_WITH_NAMESPACE>";
  private static final String topicName = "<REPLACE_WITH_TOPIC>";
  private static final String topic = String.format("persistent://%s/%s/%s", tenantName,namespace,topicName);

  public static void main(String[] args) {
----

. Add the following code that builds the configuration that will be used by both the producer and consumer:
+
[source,java]
----
    Properties config = new Properties();
    config.put("bootstrap.servers",bootstrapServers);
    config.put("security.protocol","SASL_SSL");
    config.put("sasl.jaas.config", String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username='%s' password='token:%s';", tenantName, pulsarToken));
    config.put("sasl.mechanism","PLAIN");
    config.put("session.timeout.ms","45000");
    config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, LongSerializer.class.getName());
    config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
    config.put("group.id", "my-consumer-group");
----

. Add the producer code, which is a simple flow that sends a single message and awaits acknowledgment:
+
[source,java]
----
    KafkaProducer<Long, String> producer = new KafkaProducer<>(config);

    final ProducerRecord<Long, String> producerRecord = new ProducerRecord<>(topic, System.currentTimeMillis(), "Hello World");
    producer.send(producerRecord, new Callback() {
      public void onCompletion(RecordMetadata metadata, Exception e) {
        if (e != null)
          System.out.println(String.format("Send failed for record, %s. \nRecord data: %s",e.getMessage(), producerRecord));
        else
          System.out.println("Successfully sent message");
      }
    });

    producer.flush();
    producer.close();
----

. Add the consumer code, which creates a basic subscription and retrieves the latest messages on the topic:
+
[source,java]
----
    final KafkaConsumer<Integer, String> consumer = new KafkaConsumer<Integer, String>(config);

    consumer.subscribe(Collections.singletonList(topic));
    ConsumerRecords<Integer, String> consumerRecords = consumer.poll(Duration.ofMillis(5000));

    System.out.println(String.format("Found %d total record(s)", consumerRecords.count()));

    for (ConsumerRecord<Integer, String> consumerRecord : consumerRecords) {
      System.out.println(consumerRecord);
    }

    consumer.commitSync();
    consumer.close();
  }
}
----

. Build and run a JAR file for the complete program:
+
[source,shell]
----
mvn clean package assembly:single
java -jar target/StarlightForKafkaClient-1.0-SNAPSHOT-jar-with-dependencies.jar
----
+
.Result
[%collapsible]
====
[source,shell]
----
Successfully sent message

Found 1 total record(s)
ConsumerRecord(topic = persistent://my-tenant-007/my-namespace/my-topic, partition = 0, leaderEpoch = null, offset = 22, CreateTime = 1673545962124, serialized key size = 8, serialized value size = 11, headers = RecordHeaders(headers = [], isReadOnly = false), key =   xxxxx, value = Hello World)
----
====
--
======

== See also

* xref:starlight-for-kafka:operations:starlight-kafka-kstreams.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-implementation.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-monitor.adoc[]
* xref:starlight-for-kafka:operations:starlight-kafka-security.adoc[]